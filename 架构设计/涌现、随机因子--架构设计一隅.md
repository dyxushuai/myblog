在架构设计及演进的的过程中，从各阶段来看，我们做了一系列的局部优化，看似正确，但最后众多优化动作的效果集中涌现，居然让系统朝着坏的预期发展，让架构变得脆弱。最近在关注ceph jewel10.2.11提交的一个feature，看看这个案例带给我们架构设计的借鉴意义

Ceph的数据分布设计其实是一种伪随机分布，虽然我们可以设计crush map的权重，但生产环境的osd差异不大，这意味着数据被均匀的分配到各个承载着osd的硬盘上，而且数值相当接近，非常理想的分布

PG里面的object默认为4M一个，当一个rbd的数据量剧烈增长时，filestore的一个文件夹会积累大量的object文件，影响了读写性能，为了优化这个场景，filestore选择在适当的时机将这个文件夹分裂出16个子文件夹，然后将object均匀分散到这些子文件夹中

到此为止，依然看不出有什么毛病，但集群运行一阵子后，osd会间歇性的出现slow request，原因是数据均匀分布，让各个osd的数据量均匀增长，有一定的概率让多个osd同时达到分裂值，此时osd无法继续读写，让这个集群进入block状态。拥堵的时间再长一点，可以直接让运行的虚拟机kernel crash掉。

上述一系列架构优化最终让系统变得脆弱，不被信任。社区如何解决这个问题了？答案就是让各个pg上的object分裂时机变得随机，在分裂出子文件夹时，计算出一个随机因子，并作为附加属性添加到子文件夹上，等后面子文件夹需要判断分裂时机时，取出这个值即可，我觉得在设计大型分布式系统时，需要注意这个模式设计，以免系统的表现不符合预期