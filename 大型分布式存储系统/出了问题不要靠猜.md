标题致敬道神《[出了问题不要靠猜](https://ruby-china.org/topics/14898)》

# 背景
之前做企业级产品，最坏的情况也就是出差去客户现场调试bug，客户业务也可以暂停使用。与以前做企业级产品不同，我司的业务是24小时不间断的服务，局部偶现的bug没法通过中断业务去调试，只能多看少碰。这次mongodb的慢查询陆陆续续花费了我将近3周的时间才找到原因，值得反思和总结。没时间读懂开源的每行代码，也是大规模使用开源代码的通病，所以我们只能整理一些解决问题的思路，方便快速定位问题。

# mongos定期出现大量慢查询
在mongodb集群中，mongos属于路由服务，通过cache计算出正确的分片就转发给sharding去干活了，通常消耗很少的CPU、内存资源，绝大多数性能瓶颈都出现在sharding上，所以很少被大家关注。但是，这个生产环境奇葩的地方在于sharding分片没有任何慢查询，且监控指标显示的wiredTiger cache置换速度很正常。所以一开始的思路是怀疑请求的方式由最终一致性改成强一致性导致，或者由于config server的性能瓶颈导致分片cache信息不及时，又或者是存在currentOp出现大量的lock阻塞了，不过排除掉这些业务因素后又陷入了困境，准备从系统资源入手。由于设备紧张，这个集群还混布另外几个极度耗费CPU和内存的服务（系统资源一度达到70%），怀疑资源不够导致网络通信的问题

# 系统资源排查
通过sar工具观察buffer/cache的申请和释放，以及tcp buffer的使用情况，比其他集群要高很多倍，但数值相对于服务器配置，依然不足以成为瓶颈。不得不通过tcpdump抓包，看看到底是不是网络的问题

# tcpdump
由于mongos定期刷慢查询日志，所以写了一个脚本解析日志，匹配到慢查询后就立马触发tcpdump，解析mongos和mongod的package。过滤出指定端口的package信息，发现一个来回请求很迅速，但是慢查询期间，mongos先后请求的间隔时间拉长了。至少可以彻底排除掉mongod的问题，现在重心回到mongos。

# perf
通过/proc/xx/stack、perf工具查看进程的堆栈，没有发现异常阻塞的函数。所有不碰服务的观察手段已经用完了，现在需要debug mongos。

# debug mongos日志
通常默认配置的日志为0，只输出少量的日志，无法定位问题。把日志等级调整为5，会输出大量的日志，很快就会把存储写满。于是只开了3档，2个多小时采集了18GB的日志数据。此时split、awk、jq、grep等大伙不愿意学的技巧就能派上用场了。

- 通过split工具将18GB文件拆成小文件，否则打开大文件可能导致生产系统的内存耗尽，造成二次灾难
- 通过jq、awk找到耗时较高的请求日志
- 通过grep找到日志的位置，通过context分析上下文

只找到一处短短几分钟内刷了一批耗时50-90毫秒的请求，严格来讲超过100毫秒才算慢查询。不过幸运的是这些信息也很典型，对于排查问题有些新的启发，这些请求70%的时间是花费在runCommand、establishCursors函数之间，还没有向分片发送请求。通过分析一个开了5档日志的请求，两个函数之间只输出了一些queryPlan的日志，无法帮助定位问题。现在就需要去阅读代码，看看这两个函数之间发生了什么事情

# 阅读代码
说实话，mongos的代码比较难读懂，并不是因为c++的代码难以理解，而是本身要兼容很多业务场景而使用c++的abstraction。理顺逻辑的过程中，发现代码巨量，再加上这种abstraction的调用，无法顺利的将这个流程图理出来，中间出现断层。唯一能确定的是runCommand使用到了队列，而发送请求前的task_executor消费了队列，进一步怀疑是task_excutor的消费能力。

# 阅读官网配置
去官网查询影响taskExecutor的内容，找到taskExecutorPoolSize配置，原来4.0之前的版本默认是0，即size和cpu核数一致，但是4.0开始，这个默认值变成了1，可能影响到了mongos的并发处理能力。为了迅速验证这个想法，尽快解决问题，接下来要进行对比测试

# 对比测试
将一半mongos数量的配置改成0，重启后生效。逐步把超过70%的流量切到这批mongos，当出现慢查询告警时分析日志，发现这批新配置的mongos虽然承接了70%的流量，但是没有出现慢查询，基本断定改动有效果。接下来将所有mongos改成新配置，慢查询问题彻底解决

# 复盘
事后诸葛一下，为什么这个问题花费了我3周的时间
- 本身我不是SRE，简单分析了一下问题后，觉得硬件的负载太高了，希望运维先把环境统一，先排除这个因素
- 生产环境的缘故，没办法一把梭，只能花数小时采集日志，数小时分析日志，排除干扰因素后，又得来一轮新的验证
- 针对默认配置，我们只优化了几个参数，至今没搞明白为什么官方突然要这样改默认配置，通常默认配置应该是一个比较理想的均衡配置，而这个改动完全是限制了数据库的能力
- 上线前阅读release文档，是否能提前解决这个问题？即使提前阅读到这个变更，我们是否知道它的影响
- 还是没彻底搞明白开源数据库的原理