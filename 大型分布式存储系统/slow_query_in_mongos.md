# mongos定期出现大量慢查询
在mongodb集群中，mongos属于路由服务，通过cache计算出正确的分片就转发给sharding去干活了，通常消耗很少的CPU、内存资源，绝大多数性能瓶颈都出现在sharding上，所以很少被大家关注。但是，这个生产环境奇葩的地方在于sharding分片没有任何慢查询，且监控指标显示的wiredTiger cache置换速度很正常。所以一开始的思路是怀疑请求的方式由最终一致性改成强一致性导致，或者由于config server的性能瓶颈导致分片cache信息不及时，又或者是存在currentOp出现大量的lock阻塞了，不过排除掉这些业务因素后又陷入了困境，准备从系统资源入手。由于设备紧张，这个集群还混布另外几个极度耗费CPU和内存的服务（系统资源一度达到70%），怀疑资源不够导致网络通信的问题

# 系统资源排查
通过sar工具观察buffer/cache的申请和释放，以及tcp buffer的使用情况，比其他集群要高很多倍，但数值相对于服务器配置，依然不足以成为瓶颈。不得不通过tcpdump抓包，看看到底是不是网络的问题

# tcpdump
由于mongos定期刷慢查询日志，所以写了一个脚本解析日志，匹配到慢查询后就立马触发tcpdump，解析mongos和mongod的package。过滤出指定端口的package信息，发现一个来回请求很迅速，但是慢查询期间，mongos先后请求的间隔时间拉长了。至少可以彻底排除掉mongod的问题，现在重心回到mongos。

# perf
通过/proc/xx/stack、perf工具查看进程的堆栈，没有发现异常阻塞的函数。所有不碰服务的观察手段已经用完了，现在需要debug mongos。

# debug mongos日志
通常默认配置的日志为0，只输出少量的日志，无法定位问题。把日志等级调整为5，会输出大量的日志，很快就会把存储写满。于是只开了3档，2个多小时采集了18GB的日志数据。此时split、awk、jq、grep等大伙不愿意学的技巧就能派上用场了。

- 通过split工具将18GB文件拆成小文件，否则打开大文件可能导致生产系统的内存耗尽，造成二次灾难
- 通过jq、awk找到耗时较高的请求日志
- 通过grep找到日志的位置，通过context分析上下文

只找到一处短短几分钟内刷了一批耗时50-90毫秒的请求，严格来讲超过100毫秒才算慢查询。不过幸运的是这些信息也很典型，对于排查问题有些新的启发，这些请求70%的时间是花费在runCommand、establishCursors函数之间，还没有向分片发送请求。通过分析一个开了5档日志的请求，两个函数之间只输出了一些queryPlan的日志，无法帮助定位问题。现在就需要去阅读代码，看看这两个函数之间发生了什么事情

# 阅读代码
说实话，mongos的代码比较难读懂，并不是因为c++的代码难以理解，而是本身要兼容很多业务场景而使用c++的abstraction。理顺逻辑的过程中，发现代码巨量，再加上这种abstraction的调用，无法顺利的将这个流程图理出来，中间出现断层。唯一能确定的是runCommand使用到了队列，而发送请求前的task_executor消费了队列，进一步怀疑是task_excutor的消费能力。

# 阅读官网配置
去官网查询影响taskExecutor的内容，找到taskExecutorPoolSize配置，原来4.0之前的版本默认是0，即size和cpu核数一致，但是4.0开始，这个默认值变成了1，可能影响到了mongos的并发处理能力。为了迅速验证这个想法，尽快解决问题，接下来要进行对比测试

# 对比测试
将一半mongos数量的配置改成0，重启后生效。逐步把超过70%的流量切到这批mongos，当出现慢查询告警时分析日志，发现这批新配置的mongos虽然承接了70%的流量，但是没有出现慢查询，基本断定改动有效果。接下来将所有mongos改成新配置，慢查询问题彻底解决