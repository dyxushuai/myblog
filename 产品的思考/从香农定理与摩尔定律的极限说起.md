最近任正非老爷子发文表达对未来的焦虑，因为我们即将达到[香农定理](https://baike.baidu.com/item/%E9%A6%99%E5%86%9C%E4%B8%89%E5%A4%A7%E5%AE%9A%E7%90%86/9029931)与[摩尔定律](https://baike.baidu.com/item/%E6%91%A9%E5%B0%94%E5%AE%9A%E7%90%86)的极限，这意味着我们的硬件研发将达到现有基础科学理论上的瓶颈，即使继续加大研发，也是边际递减，尤其是在10nm及以后，在设计方面，布线阻塞的问题越来越大，并且RC延迟、电迁移以及热、静电放电和电磁干扰等物理效应还在加剧。

### 突破摩尔定律
回顾历史创新的突破，往往不是沿袭旧有的研发思路，而是跨界打击，降维打击，比如量子计算机，各个国家在这个领域扳手腕，试图在[量子计算机](https://baike.baidu.com/item/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97%E6%9C%BA)领域获得突破，我相信在不久的将来，通过量子计算机，我们在计算与通信（跨国数据传输、量子同步）领域会有跨世纪的突破。当然，这种技术研发的成本让一些小玩家无法上牌桌。

### 硬件红利消失到编程优化
过去摩尔定律有效时，我们不太在乎软件的性能，能用堆硬件的方式解决的问题坚决不会通过优化软件来
解决，而如今大厂基于硬件成本的考虑，往往会选择通过顶级工程师的创造性，设计出*基于廉价硬件的分布式软件来解决指数增长的硬件和数据*，比如说google的google file system、阿里巴巴的去IOE化

*软件层次的减少*，过去，我们通过软件设计，将一个个复杂的系统拆解、分层，每个层次解决各自的问题。如今，我们也在尝试减少分层，来突破冯诺伊曼的计算机架构，比如说RDMA通信技术，直接将数据写入到远端服务器内存，减少原先设计的软件在七层网络中的损耗，这些都是软件开发的自我革命

*如何在30秒内恢复一块7200转/s的1TB机械硬盘*?前辈出了一个考题，我们觉得不可能，因为最快200M/s的写入速度也需要1.5小时，可30秒恢复一块硬盘的数据是什么鬼？感觉反常识、反科学，后来公布答案的时候就豁然了，原来那块硬盘的数据是切片数据，硬盘存了切片的三副本之一，当硬盘坏掉之后，其他副本会寻找新的硬盘，复制一份过去，相当于一个并行拷贝的过程

上述例子足够说明，我们在*软件设计层面还可以继续创新*。其解决的思路基本上是用足够多的廉价机器支撑起并行计算，当然，这也要求业务是stateless的，任意节点crash，也不会影响业务的一致性

最近谷歌云负面新闻比较多，我只能理解为k8ts模式暂时落后于openstack模式，我个人认为borg的设计理念才是云的终极形态，只是现有企业的IT基础设施和软件系统还只能适应传统的虚拟机模式，像这些头部的互联网公司，自己逐步将自己的基础设施改造成SOA或者说微服务模式，这样一来，运维的复杂度大大降低

### 能被利用的数据才是黄金
物联网时代，各种传感器收集了大量数据，如果无法利用，只会成为负担，小厂的利润基本会被数据存储的成本吞噬。我的几位朋友创业，在搞农业大数据，其中有项技术是通过传感器去收集农用机车的耗油量，防止司机偷油，由于存储成本过高，他们只保留三天的数据去分析，我只能说短时间维度思考是对的，但如果把维度拉伸到三年到五年，我们甚至可以分析出机车是否需要换代、维修，而不是被司机骗维修补贴，并且我们还可以将油耗作为一个标签，分析出司机收割单位面积的农作物的效率是什么状况。

因此，在开发数据存储时，能否利用香农定理来优化数据，通过数据清洗、压缩，确保数据失帧的情况下，依旧能被利用，有效的节省成本。

纠删码替代三副本，我第一次听说分布式存储使用纠删码是七牛在QCON上分享出来的，据介绍，1.3倍的存储就能保证数据的高可用，我相信这个技术为七牛接下来的发展提供了高速路，因为成本大大降低了，而他们只需要解决纠删码的并行计算问题。如今ceph也提供了纠删码，可是性能达不到要求。

*技术创新带来的额外财富*

### 开源软件撑起来的世界
感谢开源软件，让我们50+人的团队能正常运营三个云计算中心。但遗憾的是，软件几乎全部来自美国，我当然希望国人自强，然而传言我们仰望的清华、北大，只是美国常春藤、牛津、剑桥等高校的预备班，人才如何竞争？

### 站在巨人的肩膀上
上述个人观点，来自我从事的软件开发行业，有的是权威调研，有的是媒体文章，但这些信息有可能是失真的，比如paper或者news故意夸大成果，吸引眼球，或者避免泄密、避讳而表述不尽其意，都会影响读者的判断。但我依然坚信，*站在巨人的肩上*，走在创新的边沿，才能跨过阻碍前行的沟壑，最终通过创新，推动人类进步
