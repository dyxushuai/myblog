疫情的原因导致我们不得不在家隔离很长时间，再者很长时间也找不到烧脑的美剧了，我决定利用这段时间突破一下望而生畏的机器学习，说实话，之前买了几本机器学习的书，跟了几集吴恩达在斯坦福的课程，无一例外地在做数学推导，看着这些陌生的数学符号以及莫名其妙的公式变形，我内心是崩溃的，所以我决定先复习一下大学的课程

在这里，我要先怒赞一波B站，我没有深度调研过B站up主的奖励机制，但是这个网站相比于爱优腾，充满了活力，激发了up主的创造力，最重要的是什么？up主会和你互动......这个网站是活的......

推荐B站这几个课程：
- 墙裂推荐宋浩老师的同济版《高等数学》，粉笔板书，相比于其他老师的PPT教学，2倍速度能跟上[课程](https://www.bilibili.com/video/av48624233)
- 同时推荐宋浩老师的《线性代数》、《概率统计》
- 上面三门课程刷完后，推荐浙江大学研究生课程：《[机器学习课程](https://www.bilibili.com/video/av82557490)》，这位老师也是粉笔板书，但最重要的是他会告诉你推导的时候哪个地方最难理解（他当时花了很长时间才能理解的内容），然后他会在这个地方花很长时间由简入繁的举例
- 上面的机器学习课程有时候推导转换太快，无法理解，这个时候可以看这个牛逼的[up主](https://www.bilibili.com/video/av31989606)，全粉笔板书，推导整个过程，还能举一反三

刷完上面的视频，基本上顿悟机器学习是怎么个玩法了。

# 线性回归
这个可能是入门级的算法，但是它隐含了机器学习的所有套路，举个例子，我们用线性回归来预测房价，假设房价只和房子的面积呈线性相关，我们需要找到这个方程：
```
Y=aX+b
```
其中X和Y是我们的样本数据，那么我们取(X1,Y1)、(X2,Y2)就能算出a、b，如此简单？图样图森破......

因为房价和房子面积是有浮动的，样本数据越多，他们越不可能在同一条直线上，所以既然样本数据是绝对正确的，那么我们需要找到一条直线，让他恰好在这些点中间，我们高中学过怎么让算式的结果误差最小，可以用均方误差来表述，或者可以看成点到这个直线的距离之和最小。请注意，目前我所学习的机器学习算法都在试图找到a、和b这样的系数或者常量，但绝对不可能找到a和b，让所有样本数据都在这条线上，如果真被你找到了，你就要开始怀疑这个过程的正确性了。

数学的意义在于巧合，让那些初看起来不可能知道答案的问题通过引入一些已知条件让不可能转换成可能，机器学习算法妙就妙在这里存在大量的数学巧合。

一个预测的场景转换成数学的线性问题，看似很难找到a、b的问题转化成求均方误差最小的问题，均方误差最小的求解又可以通过最小二乘法求解。分别对a、b求偏导，导数为0，值最小。

我们已知的条件是什么？就是大量的样本X、Y，于是我们可以用矩阵（向量）的方式简化X、Y的表达，最后a的值可以用X、Y的矩阵和向量表达式计算得出来，然后再算出b，线性回归是数据完备的。

再多补充一句，XY的样本不一样，或者样本里面有脏数据，a、b的值想去甚远。因为是数学完备的，所以剩下的工作就是调参了，至此，终于明白调参党的梗是什么含义了。纯粹变成体力活，时间、计算资源的消耗战了。

# 支持向量机
支持向量机（SVM）是一个苏联人在80年代发明的，苏联解体后去了美国，1995年第一次发表了支持向量机的论文，引起轰动。支持向量机用来做分类的，应用场景比较广泛，比如车牌识别，就是先通过RNN网络提取车牌的数字和字母，再用SVM识别这些数字和字母，SVM是目前我见过数学推导最漂亮的算法，用了好几个巧合，来转化问题。如果我们不是直接看结论，我都不敢相信能找到这种巧合，这个苏联人太bug了。

怎么回事了？比如说把你的样本数据分成两类A和B，在一个平面上，用一条线把二者分隔开，就此结束？这样的线有千千万万条，哪一条最合适？高中数学是让我们求精确的值，而高等数学教我们的是求最近似的值。条件不够，精确值难求，但近似值可求，只不过误差有大有小。怎么求这条线，怎么让误差最小，转化成我们高中学过向量知识，A类的点到这条直线的距离为r，B类的点到直线的距离为r，如果2r的值最大，这条直线就是最合适的。那么分类问题就变成一个向量问题，再加上约束条件，稍微做一下变形，就可以用拉格朗日极值法求解，但拉格朗日求值得解法还是不够，变形后又转换成了一个对偶问题，对偶问题隐含的KKT条件可以简化问题。但我们的数据通常无法用一条直线分割，因此我们可以把这个数据看成是一个高维空间的数据再低一维数据的一种映射，因此我们又可以利用核函数的特性让数据升维再降维，转换公式后，一些特性让我们避开高维的计算

拉格朗日极值法、对偶问题、和函数成为解决SVM的关键，一个看似不可能的问题被化解了，这就是数学的奥妙

# 神经网络
神经网络一度式微，但LeCun开创的卷积神经网络让这一派燃起了希望，2013年AlexNet的出现直接甩开传统机器学习十几个点，但说实话AlexNet并没有什么开创性的理论突破，只是比LeCun的神经网络的设计多一些隐藏层，目前已知最多的是156层的神经网络。直到增强学习的alphaGo刷爆眼球，掀起AI热，说实话有点过热了。我觉得目前的机器学习离强人工智能还差了十万八千里，但卷积神经网络确实能基本解决图像分类的问题。

神经网络的数学是不完备的，你搞懂了后向传播算法，基本上就搞懂了神经网络是怎么训练的，相比入SVM的优雅，让我震撼，神经网络的算法推导平淡无奇，而后向传播算法的核心也仅仅是一个梯度下降的问题。神经网络如何设计，隐藏层设计多少层，激活函数怎么选，设计多少个激活函数，这些都没有一个依据，缺乏数学上的完美解释，因此，更多的是实验性的，瞎猜的，然后拼算力，不停的实验，你要说我的神经网络为什么好，不知道，但是实验的结果很好，仅此而已，反而更像一门玄学。

以卷积神经为例，我们看看神经网络是如何调参，CNN的模型是固定的，但是里面的参数不可知，需要拿标注好的数据去计算出这些参数，后向传播算法怎么解决参数的问题了？

- 第一步居然是随机初始化这些参数......请注意：`随机`
- 通过带入样本数据（X,Y）计算出所有的常量
- 利用均值方差最小，求某层神经元的参数更新，怎么更新？链式求导得出梯度乘以步长，更新参数
- 反复上述2、3步，直至均值方差最小

so easy?

学术上，目前机器学习的理论基础基本上是这个样子了，LeCun之后再无重大突破，传统的机器学习算法几无技巧的更新，神经网络的活力会好些，但也是修修补补，小打小闹，都在为了从96%的识别率提升1个百分点而努力。而大量的从业人员都在调参，通过以上的描述，大家应该懂什么叫调参了吧。说实话，有很多框架已经实现了上述算法，可能不到500行的代码便可以写出一个卷积神经网络出来，剩下的就交给硬件和框架了。

奥卡姆剃刀原理是如无必要，勿增实体，但对于神经网络来讲，设计得越复杂越好，但复杂到一定程度，计算的结果又不准了，所以神经网络更多的依赖是实验和经验，数学不完备，可以说是一门玄学。另外墙裂推荐一个论文网站：https://arxiv.org/ ，很多大牛的文章一时半会儿无法在各类期刊上发表论文，所以他们会把有瑕疵的文章发到这里来占坑，免费的哦。据说吴恩达每天会打印这些论文，随身携带，方便查阅。神经网络还能留下点念想

# end
如果你看到这篇文章，而且乐意看到这一段，我就再说点心得，其实机器学习的数据推导不难理解，因为人家已经推导出来了，你只需要理解人家的推导、公式转换的依据是什么，比做题目轻松太多。而且人家的依据是书本上已知的结论，你只需要弄懂那些符号的含义是什么。数学上如果你不乐意完整的刷完大学的课程，你可以着重看这几章：《高等数学》微分，《线性代数》矩阵、向量、转秩、梯度下降，《概率统计》的分布

机器学习的几个核心问题：
- 算法已知，你没有数据
- 芯片很贵，搭个集群去训练模型费钱，一般人扛不住，所以说芯片是门好生意？
- 各类ML框架没解决一个并行计算的问题，如何把一个问题分摊到集群，加速计算结果？算法上能否拆解？
- 通过推广自己开发的框架绑定自己的硬件？
- 数据量很大，如何存储？如何快速加载？
- 云计算公司按时租赁GPU给你训练模型，数据的安全如何保障？
- 非结构化数据转结构化数据？

我从软件开发的角度来讲，机器学习可能更多的是工程问题，学术上的突破还是交给业界大牛吧。希望在这场AI竞赛上，我们能真正意义上和美帝抗衡，重视基础理论的研究和突破，加快计算型芯片的开发等，自己好歹也搞出个软件框架出来，而不单单是大量的调参党只做应用层创新，不然喊喊厉害了我的国，然后就没打回原形
